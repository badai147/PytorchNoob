{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c432305",
   "metadata": {},
   "source": [
    "# Seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce867cb1",
   "metadata": {},
   "source": [
    "### 1. 介绍\n",
    "Seq2Seq模型是输出的长度不确定时采用的模型，能够将序列作为输入并输出不定长的输出，一般在机器翻译等领域中使用。\n",
    "\n",
    "论文链接：https://arxiv.org/pdf/1409.3215"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d999360",
   "metadata": {},
   "source": [
    "### 2. 模型架构\n",
    "#### 编码器Encoder\n",
    "将输入序列压缩为一个定长的向量，这个向量包含着输入序列的语义。\n",
    "\n",
    "#### 解码器Decorder\n",
    "将编码器输出的向量作为输入，生成指定的序列。 \n",
    "\n",
    "Seq2Seq通过编码器-解码器框架，将序列转换任务分解为理解和生成两个阶段，\n",
    "配合LSTM对长序列的建模能力，使机器翻译质量实现了质的飞跃，为后续的Transformer架构奠定了基础。\n",
    "\n",
    "#### 注意力机制Attention\n",
    "注意力机制可以有效改进Seq2seq模型，它主要思想是结合编码器中每个隐藏状态与解码器当前状态，通过计算它们之间的相关性分数来决定在生成每个目标词时应该重点关注源序列的哪些部分。原论文并没有使用Attention，但是Attention和Seq2seq的结合为后续Transformer的提出奠定了基础。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ca86dd",
   "metadata": {},
   "source": [
    "### 3. 代码实现\n",
    "我们将代码分为带Attention版本与不带Attention版本。\n",
    "\n",
    "下列版本不带Attention："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c88aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, num_layers, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "    \n",
    "    def forward(self, src):\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        _, (hidden, cell) = self.rnn(embedded)\n",
    "        return hidden, cell\n",
    "\n",
    "class Decorder(nn.Module):\n",
    "    def __init__(self, output_dim, embedding_dim, hidden_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(output_dim, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout)\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "     \n",
    "    def forward(self, input, hidden, cell):\n",
    "        input = input.unsqueeze(0)\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        pred = self.fc_out(output.squeeze(0))\n",
    "        return pred, hidden, cell\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        assert encoder.hidden_dim == decoder.hidden_dim\n",
    "        assert encoder.n_layers == decoder.n_layers\n",
    "        \n",
    "    def forward(self, src, trg):\n",
    "        batch_size = trg.shape[1]\n",
    "        trg_length = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        outputs = torch.zeros(trg_length, batch_size, trg_vocab_size)\n",
    "        hidden, cell = self.encoder(src)\n",
    "        input = trg[0,:] # 使用目标序列的第一个token\n",
    "        \n",
    "        for t in range(1, trg_length):\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            outputs[t] = output\n",
    "            input = trg[t]\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7501cff2",
   "metadata": {},
   "source": [
    "带有Attention机制的版本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e2decb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, num_layers, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "    \n",
    "    def forward(self, src):\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        outputs, (hidden, cell) = self.rnn(embedded) # [修改处]1\n",
    "        return outputs, hidden, cell\n",
    "\n",
    "class Decorder(nn.Module):\n",
    "    def __init__(self, output_dim, embedding_dim, hidden_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        # [修改处]2\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attn = nn.Linear(hidden_dim * 2, 1)\n",
    "        self.rnn = nn.LSTM(embedding_dim + hidden_dim, hidden_dim, n_layers, dropout=dropout)\n",
    "        self.fc_out = nn.Linear(hidden_dim * 2 + embedding_dim, output_dim)\n",
    "     \n",
    "    def forward(self, input, hidden, cell, encoder_outputs): # [修改处]3\n",
    "        input = input.unsqueeze(0)\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        \n",
    "        # [修改处]4\n",
    "        hidden_last = hidden[-1]\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "        hidden_repeated = hidden_last.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        encoder_outputs_reshaped = encoder_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        # 将当前解码状态与每个编码器输出的组合作为注意力机制的输入\n",
    "        attn_input = torch.cat((hidden_repeated, encoder_outputs_reshaped), dim=2)\n",
    "        attn_scores = self.attn(attn_input).squeeze(2)\n",
    "        attn_weights = F.softmax(attn_scores, dim=1)\n",
    "        \n",
    "        # 计算上下文向量\n",
    "        context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs_reshaped) # 批量矩阵乘法\n",
    "        context = context.permute(1, 0, 2)\n",
    "        \n",
    "        # 将编码向量和上下文向量的组合作为RNN(LSTM)的输入\n",
    "        rnn_input = torch.cat((embedded, context), dim=2)\n",
    "        output, (hidden, cell) = self.rnn(rnn_input, (hidden, cell))\n",
    "        \n",
    "        # 结合RNN的输出、编码向量和上下文向量，一起作为全连接层的输入\n",
    "        # 可以让神经网络学习到更多关于句子内部词语之间的关系。\n",
    "        output = output.squeeze(0)\n",
    "        embedded = embedded.squeeze(0)\n",
    "        context = context.squeeze(0)\n",
    "        pred = self.fc_out(torch.cat((output, context, embedded), dim=1))\n",
    "        \n",
    "        return pred, hidden, cell\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, src, trg):\n",
    "        batch_size = trg.shape[1]\n",
    "        trg_length = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        outputs = torch.zeros(trg_length, batch_size, trg_vocab_size)\n",
    "        encoder_outputs, hidden, cell = self.encoder(src)\n",
    "        input = trg[0,:] # 使用目标序列的第一个token\n",
    "        \n",
    "        for t in range(1, trg_length):\n",
    "            pred, hidden, cell = self.decoder(\n",
    "                input, hidden, cell, encoder_outputs\n",
    "            )\n",
    "            outputs[t] = pred\n",
    "            input = pred.argmax(1)\n",
    "        \n",
    "        return outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
