{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf01ae7b",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "\n",
    "### 1. 简介\n",
    "Transformer 架构是2017年提出的一种基于自注意力机制的深度学习模型，它彻底改变了自然语言处理领域，并成为现代大语言模型的核心基础。\n",
    "\n",
    "论文链接: https://arxiv.org/abs/1706.03762"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a99df7",
   "metadata": {},
   "source": [
    "### 2. 模型架构\n",
    "\n",
    "#### 位置编码 positional encoding\n",
    "Transformer完全基于注意力机制实现语义捕捉，没有使用到循环神经网络，因此无法捕捉序列顺序（位置）信息，\n",
    "位置编码是对序列的位置进行编码，使位置成为一个向量，这个向量与序列的内容无关，只和序列长度相关。\n",
    "其公式如下：\n",
    "\n",
    "$ PE(pos, 2i) = sin(\\frac {pos}{10000 ^ {\\frac {2i}{d}}}) $\n",
    "\n",
    "$ PE(pos, 2i+1) = cos(\\frac {pos}{10000 ^ {\\frac {2i}{d}}}) $\n",
    "\n",
    "公式解释：假设我们有五个单词组成的序列，我们使用d=4的嵌入维度的向量来表示一个单词，\n",
    "那么假设第一个单词的嵌入向量是[0.011, 0.025, 0.023, 0.256]，长度为4，\n",
    "在Transofomer架构中，为了让位置向量和嵌入向量能够相加，位置向量和嵌入向量的长度相同，\n",
    "因此位置向量的长度应该为4，由于是第一个单词，在序列的位置中为0，所以pos=0，\n",
    "即pos表示序列的绝对位置，2i和2i+1的作用仅仅是为了分出奇偶，\n",
    "\n",
    "我们计算当i=0时的结果：2i=0, 2i+1=1, 即PE(pos=0, 2i=0) = 0，PE(pos=0, 2i=1) = 1，\n",
    "即此时有两个值：0和1，\n",
    "\n",
    "我们再来计算当i=1时的结果：2i=2, 2i+1=3，即PE(pos=0, 2i=2) = 0，\n",
    "PE(pos=0, 2i+1=3) = 1，即此时有四个值：0，1，0，1。\n",
    "\n",
    "此时值的数量已经足够，不必再往下算，因此位置向量为[0, 1, 0, 1]，\n",
    "相应的，当我们计算第二个单词的位置向量时，其pos=1，有位置向量[0.8415, 0.5403, 0.0001, 1.0000]，\n",
    "这两个向量可以均可以与所在位置单词的嵌入向量相加。\n",
    "得到的结果便同时存在单词本身信息与位置信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9cbffb",
   "metadata": {},
   "source": [
    "#### 注意力机制\n",
    "注意力机制通过计算查询向量和一组键值对的相关性来工作。根据Q、K、V的来源不同，主要分为两种：\n",
    "\n",
    "自注意力（self-attention）：Q、K、V全部来源于同一个序列。例如，Transformer的Encoder中，每个词都会关注输入句子中的所有词，以充分理解上下文；在Decoder中，也会有一个掩码自注意力层，让每个词关注它之前的所有词。\n",
    "\n",
    "掩码自注意力（masked self-attention）：与自注意力相同，但是会对未来位置进行掩盖，以防止信息泄露。\n",
    "同时它还让transformer具备了并行计算的能力，如果像RNN一样逐词训练，则不具备该能力。\n",
    "\n",
    "交叉注意力：Q来源于一个序列，而K、V来源于另一个序列。这主要用于Encoder-Decoder架构，典型例子是Transformer的Decoder层，在生成目标词时，其Q会与来自Encoder输出的键值对K, V进行计算，从而聚焦于源序列中最相关的部分来进行翻译或生成。\n",
    "\n",
    "多头注意力：多个自注意力的结合。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096cf767",
   "metadata": {},
   "source": [
    "#### 层归一化 Layer Normalization\n",
    "不同于批归一化（Batch Normalization）是对整个批次的每个特征进行归一化，\n",
    "\n",
    "层归一化是对每一行进行归一操作，即对每一个样本进行归一化操作，更加适用于RNN和Transformer。\n",
    "\n",
    "例如有数据如下所示：\n",
    "\n",
    "[\n",
    "[1, 2, 3, 4],\n",
    "[5, 6, 7, 8]\n",
    "]\n",
    "\n",
    "层归一化对[1, 2, 3, 4]进行归一化，批归一化对[1, 5]进行归一化。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854d124b",
   "metadata": {},
   "source": [
    "### 3. 代码实现\n",
    "代码来源于哈佛NLP团队：https://github.com/harvardnlp/annotated-transformer\n",
    "\n",
    "代码较难理解，可结合网络各种资源和AI进行学习。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073d446d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.nn.functional import log_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcccc152",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"标准的编码器-解码器架构。\"\n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        return self.encoder(self.src_embed(src), src_mask)\n",
    "\n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef328116",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    '''\n",
    "    标准的全连接层+softmax层生成器。\n",
    "    是编码器-解码器架构的输出层，\n",
    "    用于将解码器的输出转换为词汇表上的概率分布。\n",
    "    '''\n",
    "\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646af305",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"层归一化。\"\n",
    "\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
    "\n",
    "'''\n",
    "Pytorch中有内置函数如下：\n",
    "import torch.nn as nn\n",
    "builtin_ln = nn.LayerNorm(normalized_shape=shape, eps=eps)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f49e157",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    \"克隆N个相同层。\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d09cb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    \"子连接层。残差连接+归一化，即Add & Norm层\"\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1800b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"编码器层\"\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc731ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"解码器\"\n",
    "    def __init__(self, layer, N):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29539e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"解码器层\"\n",
    "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn # 带掩码的自注意力\n",
    "        self.src_attn = src_attn # 编码器-解码器注意力\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        m = memory\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
    "        return self.sublayer[2](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79da3edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    '''\n",
    "    创建一个下三角矩阵\n",
    "    用于确保解码器在预测当前位置时只能看到当前位置及之前的位置，\n",
    "    不能看到未来的位置。\n",
    "    '''\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1).type(\n",
    "        torch.uint8\n",
    "    )\n",
    "    return subsequent_mask == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb57523",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"注意力机制\"\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = scores.softmax(dim=-1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da8384c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        \"多头注意力机制\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "\n",
    "        query, key, value = [\n",
    "            lin(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "            for lin, x in zip(self.linears, (query, key, value))\n",
    "        ]\n",
    "\n",
    "        x, self.attn = attention(\n",
    "            query, key, value, mask=mask, dropout=self.dropout\n",
    "        )\n",
    "\n",
    "        x = (\n",
    "            x.transpose(1, 2)\n",
    "            .contiguous()\n",
    "            .view(nbatches, -1, self.h * self.d_k)\n",
    "        )\n",
    "        del query\n",
    "        del key\n",
    "        del value\n",
    "        return self.linears[-1](x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243a32a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"位置前馈网络\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(self.w_1(x).relu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7279ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c943c9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"位置编码层\"\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe) # 注册为buffer，不参与训练\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb5cc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(\n",
    "    src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1\n",
    "):\n",
    "    c = copy.deepcopy # 深拷贝，确保每个层有独立的参数实例\n",
    "    attn = MultiHeadedAttention(h, d_model)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    position = PositionalEncoding(d_model, dropout)\n",
    "    model = EncoderDecoder(\n",
    "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),\n",
    "        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n",
    "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
    "        Generator(d_model, tgt_vocab), # 线性层+softmax\n",
    "    )\n",
    "\n",
    "    # Xavier均匀初始化\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
